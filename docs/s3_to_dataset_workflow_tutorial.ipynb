{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NDP EP Tutorial: S3 Storage to Dataset Registration Workflow\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sci-ndp/pop/blob/main/docs/s3_to_dataset_workflow_tutorial.ipynb)\n",
    "[![Open in Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/sci-ndp/pop/main?filepath=docs/s3_to_dataset_workflow_tutorial.ipynb)\n",
    "\n",
    "> üöÄ **Run Online Options:**\n",
    "> - **Google Colab**: Dependencies installed automatically in the first cell\n",
    "> - **Binder**: Pre-configured environment, ready to run immediately\n",
    "> - **Local**: Requires `pip install requests jupyter`\n",
    "\n",
    "This notebook demonstrates a complete workflow for scientific data management using the NDP EP API:\n",
    "\n",
    "1. **Upload files to S3 storage** using MINIO endpoints\n",
    "2. **Generate presigned URLs** for secure access\n",
    "3. **Register datasets** with S3 URLs as resources\n",
    "4. **Manage the complete data lifecycle** from storage to discovery\n",
    "\n",
    "## Use Cases\n",
    "\n",
    "This workflow is perfect for:\n",
    "- **Research data management**: Store large datasets in S3 and register them for discovery\n",
    "- **Reproducible science**: Create permanent links to data files with rich metadata\n",
    "- **Data publishing**: Combine storage with catalog registration for data sharing\n",
    "- **Institutional repositories**: Manage both storage and metadata in one workflow\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Python 3.7+\n",
    "- `requests` library\n",
    "- Access to a NDP EP API instance with S3/MINIO configured\n",
    "- Valid authentication credentials\n",
    "- Data files to upload\n",
    "\n",
    "## Workflow Overview\n",
    "\n",
    "```\n",
    "Local File ‚Üí S3 Upload ‚Üí Generate URL ‚Üí Register Dataset ‚Üí Published Data\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install requests -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n",
    "\n",
    "First, let's import the necessary libraries and configure our API connection parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from typing import Dict, Any, Optional\n",
    "import time\n",
    "import io\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Variables\n",
    "\n",
    "**Important:** Replace these values with your actual API endpoint and credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Configuration\n",
    "API_BASE_URL = \"http://localhost:8000\"  # Replace with your API URL\n",
    "\n",
    "# Authentication Token\n",
    "AUTH_TOKEN = \"testing_token\"  # Replace with your actual token\n",
    "\n",
    "# Request headers with authentication\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {AUTH_TOKEN}\",\n",
    "    \"Accept\": \"application/json\"\n",
    "}\n",
    "\n",
    "print(f\"API Base URL: {API_BASE_URL}\")\n",
    "print(f\"Token configured: {'‚úì' if AUTH_TOKEN != 'your_auth_token_here' else '‚úó Please set your token'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "\n",
    "Let's create utility functions for both S3 and dataset operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_api_request(method: str, endpoint: str, data: Optional[Dict] = None, \n",
    "                    params: Optional[Dict] = None, files: Optional[Dict] = None,\n",
    "                    custom_headers: Optional[Dict] = None) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Make an API request with proper error handling for both S3 and dataset endpoints.\n",
    "    \"\"\"\n",
    "    url = f\"{API_BASE_URL}{endpoint}\"\n",
    "    \n",
    "    # Prepare headers\n",
    "    headers = HEADERS.copy()\n",
    "    if custom_headers:\n",
    "        headers.update(custom_headers)\n",
    "    \n",
    "    # Remove Content-Type for file uploads to let requests set it\n",
    "    if files and \"Content-Type\" in headers:\n",
    "        del headers[\"Content-Type\"]\n",
    "    \n",
    "    try:\n",
    "        response = requests.request(\n",
    "            method=method,\n",
    "            url=url,\n",
    "            headers=headers,\n",
    "            json=data if not files else None,\n",
    "            data=data if files else None,\n",
    "            files=files,\n",
    "            params=params,\n",
    "            stream=(method == \"GET\" and \"download\" in endpoint.lower())\n",
    "        )\n",
    "        \n",
    "        print(f\"üîó {method} {url}\")\n",
    "        print(f\"üìä Status: {response.status_code}\")\n",
    "        \n",
    "        if response.status_code in [200, 201, 204]:\n",
    "            # Handle streaming responses (file downloads)\n",
    "            if response.headers.get('content-type', '').startswith('application/octet-stream') or \\\n",
    "               'attachment' in response.headers.get('content-disposition', ''):\n",
    "                print(\"‚úÖ Success! (File download)\")\n",
    "                return {\n",
    "                    \"success\": True,\n",
    "                    \"content\": response.content,\n",
    "                    \"headers\": dict(response.headers),\n",
    "                    \"status_code\": response.status_code\n",
    "                }\n",
    "            \n",
    "            # Handle JSON responses\n",
    "            try:\n",
    "                result = response.json()\n",
    "                print(\"‚úÖ Success!\")\n",
    "                return result\n",
    "            except ValueError:\n",
    "                # Handle non-JSON success responses\n",
    "                print(\"‚úÖ Success! (Non-JSON response)\")\n",
    "                return {\"success\": True, \"status_code\": response.status_code}\n",
    "        else:\n",
    "            print(f\"‚ùå Error: {response.status_code}\")\n",
    "            try:\n",
    "                error_detail = response.json()\n",
    "                print(f\"Error details: {json.dumps(error_detail, indent=2)}\")\n",
    "            except:\n",
    "                print(f\"Error text: {response.text}\")\n",
    "            return {\"error\": True, \"status_code\": response.status_code, \"detail\": response.text}\n",
    "            \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"‚ùå Request failed: {e}\")\n",
    "        return {\"error\": True, \"exception\": str(e)}\n",
    "\n",
    "def print_response(response: Dict[str, Any], title: str = \"Response\"):\n",
    "    \"\"\"Pretty print API responses.\"\"\"\n",
    "    print(f\"\\nüìã {title}:\")\n",
    "    print(\"‚îÄ\" * 50)\n",
    "    if \"content\" in response:  # File download response\n",
    "        print(f\"File size: {len(response['content'])} bytes\")\n",
    "        print(f\"Headers: {response['headers']}\")\n",
    "    else:\n",
    "        pprint(response)\n",
    "    print(\"‚îÄ\" * 50)\n",
    "\n",
    "def create_sample_file(filename: str, content: str = None, size_kb: int = None) -> str:\n",
    "    \"\"\"Create a sample file for upload testing.\"\"\"\n",
    "    if content is None:\n",
    "        if size_kb:\n",
    "            # Create file of specific size\n",
    "            content = \"Sample data line for workflow tutorial.\\n\" * (size_kb * 25)  # Approx 1KB per 25 lines\n",
    "        else:\n",
    "            content = f\"\"\"# Research Data File - {filename}\n",
    "# Created: {datetime.now().isoformat()}\n",
    "# Purpose: NDP EP S3-to-Dataset workflow tutorial\n",
    "\n",
    "timestamp,temperature,humidity,pressure\n",
    "2024-01-01T00:00:00Z,23.5,45.2,1013.25\n",
    "2024-01-01T01:00:00Z,23.2,46.1,1013.15\n",
    "2024-01-01T02:00:00Z,22.8,47.3,1012.98\n",
    "2024-01-01T03:00:00Z,22.4,48.0,1012.75\n",
    "2024-01-01T04:00:00Z,22.1,48.9,1012.60\n",
    "\n",
    "# This is sample weather data for the tutorial\n",
    "# In a real workflow, this would be your actual research data\n",
    "\"\"\"\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(content)\n",
    "    \n",
    "    size_bytes = os.path.getsize(filename)\n",
    "    print(f\"üìÑ Created sample file: {filename} ({size_bytes} bytes)\")\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. API Connectivity Test\n",
    "\n",
    "Let's verify that both S3 and dataset endpoints are accessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test API connectivity\n",
    "print(\"üß™ Testing API connectivity...\")\n",
    "status_response = make_api_request(\"GET\", \"/status/\")\n",
    "print_response(status_response, \"API Status\")\n",
    "\n",
    "if \"error\" not in status_response:\n",
    "    print(\"üéâ API is accessible and responding correctly!\")\n",
    "    \n",
    "    # Test S3 endpoints\n",
    "    print(\"\\nüß™ Testing S3 service...\")\n",
    "    buckets_response = make_api_request(\"GET\", \"/s3/buckets/\")\n",
    "    \n",
    "    if \"error\" not in buckets_response:\n",
    "        print(\"‚úÖ S3 service is available and configured!\")\n",
    "        print(f\"üì¶ Found {len(buckets_response.get('buckets', []))} existing buckets\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è S3 service may not be configured\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è API connectivity issues. Please check your configuration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Step 1: Prepare and Upload Data to S3\n",
    "\n",
    "First, we'll create sample research data and upload it to S3 storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Sample Research Data\n",
    "\n",
    "Let's create some sample files that represent typical research data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate unique identifiers for this workflow\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "workflow_id = f\"workflow_{timestamp}\"\n",
    "\n",
    "print(f\"üî¨ Creating sample research data for workflow: {workflow_id}\")\n",
    "\n",
    "# Create different types of research files\n",
    "sample_files = [\n",
    "    {\n",
    "        \"filename\": f\"temperature_data_{timestamp}.csv\",\n",
    "        \"content\": f\"\"\"# Temperature Measurement Dataset\n",
    "# Workflow ID: {workflow_id}\n",
    "# Created: {datetime.now().isoformat()}\n",
    "# Instrument: Weather Station Network\n",
    "# Location: Research Site A\n",
    "\n",
    "timestamp,temperature_celsius,quality_flag,station_id\n",
    "2024-01-01T00:00:00Z,23.5,GOOD,WS001\n",
    "2024-01-01T01:00:00Z,23.2,GOOD,WS001\n",
    "2024-01-01T02:00:00Z,22.8,GOOD,WS001\n",
    "2024-01-01T03:00:00Z,22.4,SUSPECT,WS001\n",
    "2024-01-01T04:00:00Z,22.1,GOOD,WS001\n",
    "2024-01-01T05:00:00Z,21.9,GOOD,WS001\n",
    "2024-01-01T06:00:00Z,22.3,GOOD,WS001\n",
    "2024-01-01T07:00:00Z,23.1,GOOD,WS001\n",
    "2024-01-01T08:00:00Z,24.2,GOOD,WS001\n",
    "2024-01-01T09:00:00Z,25.8,GOOD,WS001\n",
    "\"\"\",\n",
    "        \"description\": \"Hourly temperature measurements with quality flags\",\n",
    "        \"format\": \"CSV\"\n",
    "    },\n",
    "    {\n",
    "        \"filename\": f\"analysis_results_{timestamp}.json\",\n",
    "        \"content\": json.dumps({\n",
    "            \"workflow_id\": workflow_id,\n",
    "            \"analysis_type\": \"statistical_summary\",\n",
    "            \"created\": datetime.now().isoformat(),\n",
    "            \"data_sources\": [\"temperature_sensors\", \"humidity_sensors\"],\n",
    "            \"results\": {\n",
    "                \"temperature_stats\": {\n",
    "                    \"mean\": 23.21,\n",
    "                    \"std_dev\": 1.15,\n",
    "                    \"min\": 21.9,\n",
    "                    \"max\": 25.8,\n",
    "                    \"n_observations\": 10\n",
    "                },\n",
    "                \"quality_assessment\": {\n",
    "                    \"good_data_percentage\": 90.0,\n",
    "                    \"suspect_data_count\": 1,\n",
    "                    \"missing_data_count\": 0\n",
    "                }\n",
    "            },\n",
    "            \"methodology\": \"Standard statistical analysis with outlier detection\",\n",
    "            \"software_version\": \"Analysis Pipeline v2.1\"\n",
    "        }, indent=2),\n",
    "        \"description\": \"Statistical analysis results in JSON format\",\n",
    "        \"format\": \"JSON\"\n",
    "    },\n",
    "    {\n",
    "        \"filename\": f\"methodology_{timestamp}.md\",\n",
    "        \"content\": f\"\"\"# Research Methodology Documentation\n",
    "\n",
    "**Workflow ID:** {workflow_id}  \n",
    "**Created:** {datetime.now().isoformat()}  \n",
    "**Study Type:** Environmental Monitoring\n",
    "\n",
    "## Overview\n",
    "\n",
    "This document describes the methodology used for collecting and analyzing environmental data as part of the NDP EP workflow tutorial.\n",
    "\n",
    "## Data Collection\n",
    "\n",
    "### Instruments\n",
    "- Weather Station Network (Model: WS-2024)\n",
    "- Temperature sensors: ¬±0.1¬∞C accuracy\n",
    "- Sampling interval: 1 hour\n",
    "- Quality control: Automated + manual review\n",
    "\n",
    "### Locations\n",
    "- Research Site A: 40.7128¬∞N, 74.0060¬∞W\n",
    "- Elevation: 10m above sea level\n",
    "- Environment: Urban research station\n",
    "\n",
    "## Data Processing\n",
    "\n",
    "1. **Raw Data Collection**\n",
    "   - Automated data logger retrieval\n",
    "   - Timestamp validation\n",
    "   - Initial quality flags\n",
    "\n",
    "2. **Quality Control**\n",
    "   - Range checks: -40¬∞C to +50¬∞C\n",
    "   - Temporal consistency checks\n",
    "   - Outlier detection using 3-sigma rule\n",
    "\n",
    "3. **Statistical Analysis**\n",
    "   - Descriptive statistics\n",
    "   - Trend analysis\n",
    "   - Quality metrics\n",
    "\n",
    "## Data Management\n",
    "\n",
    "- **Storage**: S3-compatible object storage\n",
    "- **Format**: CSV for data, JSON for analysis results\n",
    "- **Backup**: Automated daily backups\n",
    "- **Access**: Controlled via NDP EP API\n",
    "\n",
    "## References\n",
    "\n",
    "- Environmental Monitoring Standards (EMS-2024)\n",
    "- Data Quality Guidelines (DQG-v3.2)\n",
    "- NDP EP Documentation\n",
    "\"\"\",\n",
    "        \"description\": \"Comprehensive methodology documentation\",\n",
    "        \"format\": \"Markdown\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create the files\n",
    "created_files = []\n",
    "for file_info in sample_files:\n",
    "    filename = create_sample_file(file_info[\"filename\"], file_info[\"content\"])\n",
    "    created_files.append({\n",
    "        \"filename\": filename,\n",
    "        \"description\": file_info[\"description\"],\n",
    "        \"format\": file_info[\"format\"]\n",
    "    })\n",
    "\n",
    "print(f\"\\n‚úÖ Created {len(created_files)} sample files for the workflow\")\n",
    "for i, file_info in enumerate(created_files, 1):\n",
    "    print(f\"  {i}. {file_info['filename']} ({file_info['format']}) - {file_info['description']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create S3 Bucket for Research Data\n",
    "\n",
    "Now let's create a dedicated bucket for our research workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bucket for this workflow\n",
    "bucket_name = f\"research-data-{timestamp}\"\n",
    "\n",
    "print(f\"ü™£ Creating S3 bucket: {bucket_name}\")\n",
    "\n",
    "bucket_data = {\n",
    "    \"name\": bucket_name,\n",
    "    \"region\": \"us-east-1\"\n",
    "}\n",
    "\n",
    "bucket_response = make_api_request(\"POST\", \"/s3/buckets/\", data=bucket_data)\n",
    "print_response(bucket_response, \"Bucket Creation\")\n",
    "\n",
    "if \"error\" not in bucket_response:\n",
    "    print(f\"\\n‚úÖ Bucket '{bucket_name}' created successfully!\")\n",
    "    workflow_bucket = bucket_name\n",
    "else:\n",
    "    print(\"‚ùå Failed to create bucket. Using existing bucket or manual creation required.\")\n",
    "    workflow_bucket = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Research Files to S3\n",
    "\n",
    "Let's upload our research files to the S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if workflow_bucket:\n",
    "    print(f\"üì§ Uploading research files to bucket: {workflow_bucket}\")\n",
    "    \n",
    "    uploaded_files = []\n",
    "    \n",
    "    for i, file_info in enumerate(created_files, 1):\n",
    "        filename = file_info[\"filename\"]\n",
    "        print(f\"\\nüìÅ Uploading file {i}/{len(created_files)}: {filename}\")\n",
    "        \n",
    "        # Upload file\n",
    "        with open(filename, 'rb') as f:\n",
    "            files = {\"file\": (filename, f, \"text/plain\")}\n",
    "            form_data = {\"object_key\": filename}\n",
    "            \n",
    "            upload_response = make_api_request(\n",
    "                \"POST\", \n",
    "                f\"/s3/objects/{workflow_bucket}\",\n",
    "                data=form_data,\n",
    "                files=files\n",
    "            )\n",
    "            \n",
    "            if \"error\" not in upload_response:\n",
    "                print(f\"‚úÖ Uploaded: {filename}\")\n",
    "                print(f\"   üìè Size: {upload_response.get('size', 'N/A')} bytes\")\n",
    "                print(f\"   üîë Key: {upload_response.get('key', 'N/A')}\")\n",
    "                print(f\"   üì¶ Bucket: {upload_response.get('bucket', 'N/A')}\")\n",
    "                \n",
    "                uploaded_files.append({\n",
    "                    \"filename\": filename,\n",
    "                    \"bucket\": upload_response.get('bucket'),\n",
    "                    \"key\": upload_response.get('key'),\n",
    "                    \"size\": upload_response.get('size'),\n",
    "                    \"description\": file_info[\"description\"],\n",
    "                    \"format\": file_info[\"format\"]\n",
    "                })\n",
    "            else:\n",
    "                print(f\"‚ùå Failed to upload: {filename}\")\n",
    "                print_response(upload_response, \"Upload Error\")\n",
    "        \n",
    "        # Clean up local file\n",
    "        try:\n",
    "            os.remove(filename)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    print(f\"\\nüìä Upload Summary:\")\n",
    "    print(f\"‚úÖ Successfully uploaded: {len(uploaded_files)} files\")\n",
    "    print(f\"üì¶ Bucket: {workflow_bucket}\")\n",
    "    print(f\"üíæ Total size: {sum(f.get('size', 0) for f in uploaded_files)} bytes\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No bucket available for file upload\")\n",
    "    uploaded_files = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Step 2: Generate Presigned URLs\n",
    "\n",
    "Now we'll generate presigned URLs for our uploaded files. These URLs will be used as resource links in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if uploaded_files:\n",
    "    print(\"üîó Generating presigned URLs for uploaded files...\")\n",
    "    \n",
    "    # Generate presigned URLs (valid for 7 days - maximum allowed)\n",
    "    presigned_request = {\n",
    "        \"expires_in\": 604800  # 7 days in seconds (maximum)\n",
    "    }\n",
    "    \n",
    "    files_with_urls = []\n",
    "    \n",
    "    for file_info in uploaded_files:\n",
    "        bucket = file_info[\"bucket\"]\n",
    "        key = file_info[\"key\"]\n",
    "        \n",
    "        print(f\"\\nüîó Generating URL for: {key}\")\n",
    "        \n",
    "        url_response = make_api_request(\n",
    "            \"POST\",\n",
    "            f\"/s3/objects/{bucket}/{key}/presigned-download\",\n",
    "            data=presigned_request\n",
    "        )\n",
    "        \n",
    "        if \"error\" not in url_response and \"url\" in url_response:\n",
    "            print(f\"‚úÖ Generated presigned URL\")\n",
    "            print(f\"   ‚è∞ Expires in: {url_response['expires_in']} seconds ({url_response['expires_in']//3600} hours)\")\n",
    "            \n",
    "            # Add URL to file info\n",
    "            file_info_with_url = file_info.copy()\n",
    "            file_info_with_url[\"presigned_url\"] = url_response[\"url\"]\n",
    "            file_info_with_url[\"url_expires_in\"] = url_response[\"expires_in\"]\n",
    "            files_with_urls.append(file_info_with_url)\n",
    "            \n",
    "        else:\n",
    "            print(f\"‚ùå Failed to generate URL for: {key}\")\n",
    "            print_response(url_response, \"URL Generation Error\")\n",
    "    \n",
    "    print(f\"\\nüìä URL Generation Summary:\")\n",
    "    print(f\"‚úÖ Generated URLs for: {len(files_with_urls)} files\")\n",
    "    print(f\"üîó URLs valid for: {presigned_request['expires_in']//3600} hours\")\n",
    "    print(f\"üìÖ URLs expire on: {datetime.fromtimestamp(time.time() + presigned_request['expires_in'])}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No uploaded files available for URL generation\")\n",
    "    files_with_urls = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Step 3: Create Organization (if needed)\n",
    "\n",
    "Before registering datasets, we need to ensure we have an organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check existing organizations in CKAN directly\n",
    "print(\"üè¢ Checking organizations in local CKAN...\")\n",
    "\n",
    "# Get organizations directly from CKAN API\n",
    "try:\n",
    "    ckan_orgs_response = requests.get(\"http://localhost:5000/api/3/action/organization_list\")\n",
    "    if ckan_orgs_response.status_code == 200:\n",
    "        ckan_data = ckan_orgs_response.json()\n",
    "        available_orgs = ckan_data.get('result', [])\n",
    "        \n",
    "        print(f\"üìà Found {len(available_orgs)} organizations in local CKAN:\")\n",
    "        for i, org in enumerate(available_orgs, 1):\n",
    "            print(f\"  {i}. {org}\")\n",
    "        \n",
    "        # Use test_raul if available, otherwise use the first one\n",
    "        if \"test_raul\" in available_orgs:\n",
    "            organization_name = \"test_raul\"\n",
    "        elif available_orgs:\n",
    "            organization_name = available_orgs[0]\n",
    "        else:\n",
    "            organization_name = None\n",
    "            \n",
    "        if organization_name:\n",
    "            print(f\"\\n‚úÖ Using organization: {organization_name}\")\n",
    "    else:\n",
    "        print(\"‚ùå Could not retrieve organizations from CKAN\")\n",
    "        organization_name = None\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error connecting to CKAN: {e}\")\n",
    "    print(\"‚ö†Ô∏è Using fallback organization name\")\n",
    "    organization_name = \"test_raul\"  # Fallback to known working organization\n",
    "\n",
    "if not organization_name:\n",
    "    print(\"\\n‚ö†Ô∏è No organizations available. Please create one manually first.\")\n",
    "    print(\"   The tutorial requires an existing organization to register datasets.\")\n",
    "\n",
    "print(f\"\\nüéØ Organization for dataset: {organization_name or 'None available'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Step 4: Register Dataset with S3 Resources\n",
    "\n",
    "Now we'll create a comprehensive dataset that includes our S3-stored files as resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if organization_name and files_with_urls:\n",
    "    print(f\"üìä Creating dataset with S3 resources...\")\n",
    "    \n",
    "    # Prepare resources from our uploaded files\n",
    "    dataset_resources = []\n",
    "    for file_info in files_with_urls:\n",
    "        resource = {\n",
    "            \"url\": file_info[\"presigned_url\"],\n",
    "            \"name\": file_info[\"filename\"],\n",
    "            \"description\": file_info[\"description\"],\n",
    "            \"format\": file_info[\"format\"],\n",
    "            \"size\": file_info[\"size\"]\n",
    "        }\n",
    "        \n",
    "        # Add mimetype based on format\n",
    "        format_mimetypes = {\n",
    "            \"CSV\": \"text/csv\",\n",
    "            \"JSON\": \"application/json\",\n",
    "            \"Markdown\": \"text/markdown\",\n",
    "            \"PDF\": \"application/pdf\",\n",
    "            \"XML\": \"application/xml\"\n",
    "        }\n",
    "        resource[\"mimetype\"] = format_mimetypes.get(file_info[\"format\"], \"application/octet-stream\")\n",
    "        \n",
    "        dataset_resources.append(resource)\n",
    "    \n",
    "    # Create comprehensive dataset payload\n",
    "    dataset_payload = {\n",
    "        # Required fields\n",
    "        \"name\": f\"environmental_monitoring_{timestamp}\",\n",
    "        \"title\": f\"Environmental Monitoring Dataset - Workflow {workflow_id}\",\n",
    "        \"owner_org\": organization_name,\n",
    "        \n",
    "        # Descriptive metadata\n",
    "        \"notes\": f\"\"\"This dataset contains environmental monitoring data collected as part of research workflow {workflow_id}. \n",
    "\n",
    "The dataset includes:\n",
    "- Temperature measurements with quality control flags\n",
    "- Statistical analysis results\n",
    "- Comprehensive methodology documentation\n",
    "\n",
    "All data files are stored in S3 object storage and accessible via presigned URLs. This demonstrates the complete workflow from data collection through storage to dataset registration in the NDP EP platform.\n",
    "\n",
    "**Data Collection Period:** {datetime.now().strftime('%Y-%m-%d')}\n",
    "**Quality Level:** Research Grade\n",
    "**Access:** Open Access via presigned URLs\"\"\",\n",
    "        \n",
    "        # Categorization\n",
    "        \"tags\": [\n",
    "            \"environmental-monitoring\",\n",
    "            \"temperature\",\n",
    "            \"research-data\",\n",
    "            \"s3-workflow\",\n",
    "            \"quality-controlled\",\n",
    "            \"open-access\",\n",
    "            \"tutorial\"\n",
    "        ],\n",
    "        \"groups\": [\"environmental\", \"research\", \"monitoring\"],\n",
    "        \n",
    "        # Administrative metadata\n",
    "        \"license_id\": \"cc-by-4.0\",\n",
    "        \"version\": \"1.0\",\n",
    "        \"private\": False,\n",
    "        \n",
    "        # Extended metadata using extras\n",
    "        \"extras\": {\n",
    "            \"workflow_id\": workflow_id,\n",
    "            \"creation_method\": \"S3-to-Dataset API Workflow\",\n",
    "            \"storage_backend\": \"S3 Object Storage\",\n",
    "            \"bucket_name\": workflow_bucket,\n",
    "            \"data_collection_date\": datetime.now().strftime('%Y-%m-%d'),\n",
    "            \"quality_control\": \"Automated + Manual Review\",\n",
    "            \"data_format_standards\": \"CSV, JSON, Markdown\",\n",
    "            \"access_method\": \"Presigned URLs\",\n",
    "            \"url_expiration_hours\": str(presigned_request['expires_in']//3600),\n",
    "            \"geographical_coverage\": \"Research Site A (40.7128¬∞N, 74.0060¬∞W)\",\n",
    "            \"temporal_coverage\": \"2024-01-01 (sample data)\",\n",
    "            \"instrument_type\": \"Weather Station Network\",\n",
    "            \"measurement_frequency\": \"Hourly\",\n",
    "            \"data_processing_level\": \"Level 2 - Quality Controlled\",\n",
    "            \"contact_info\": \"NDP EP Tutorial\",\n",
    "            \"methodology_reference\": \"See included methodology documentation\",\n",
    "            \"software_version\": \"NDP EP API v1.0\",\n",
    "            \"backup_location\": f\"S3 bucket: {workflow_bucket}\",\n",
    "            \"checksum_algorithm\": \"MD5 (via S3 ETag)\"\n",
    "        },\n",
    "        \n",
    "        # S3-stored resources\n",
    "        \"resources\": dataset_resources\n",
    "    }\n",
    "    \n",
    "    print(f\"üìã Dataset will include:\")\n",
    "    print(f\"   üìä Name: {dataset_payload['name']}\")\n",
    "    print(f\"   üìù Title: {dataset_payload['title']}\")\n",
    "    print(f\"   üè¢ Organization: {organization_name}\")\n",
    "    print(f\"   üè∑Ô∏è Tags: {len(dataset_payload['tags'])} tags\")\n",
    "    print(f\"   üìÅ Resources: {len(dataset_resources)} S3-stored files\")\n",
    "    print(f\"   üì¶ Storage: S3 bucket '{workflow_bucket}'\")\n",
    "    print(f\"   üîó Access: Presigned URLs (valid {presigned_request['expires_in']//3600} hours)\")\n",
    "    \n",
    "    # Create the dataset\n",
    "    print(f\"\\nüöÄ Registering dataset...\")\n",
    "    dataset_response = make_api_request(\"POST\", \"/dataset\", data=dataset_payload)\n",
    "    print_response(dataset_response, \"Dataset Registration\")\n",
    "    \n",
    "    if \"error\" not in dataset_response and \"id\" in dataset_response:\n",
    "        dataset_id = dataset_response[\"id\"]\n",
    "        print(f\"\\nüéâ Dataset registered successfully!\")\n",
    "        print(f\"üÜî Dataset ID: {dataset_id}\")\n",
    "        print(f\"üìã Dataset Name: {dataset_payload['name']}\")\n",
    "        print(f\"üìÅ Resources: {len(dataset_resources)} files from S3\")\n",
    "        print(f\"üì¶ S3 Bucket: {workflow_bucket}\")\n",
    "        print(f\"üîó All files accessible via dataset resources\")\n",
    "        \n",
    "        workflow_dataset_id = dataset_id\n",
    "    else:\n",
    "        print(\"‚ùå Failed to register dataset\")\n",
    "        workflow_dataset_id = None\n",
    "        \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Cannot create dataset - missing organization or S3 files\")\n",
    "    workflow_dataset_id = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Verification and Testing\n",
    "\n",
    "Let's verify that our complete workflow worked correctly by testing access to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if workflow_dataset_id and files_with_urls:\n",
    "    print(\"üß™ Verifying workflow completion...\")\n",
    "    \n",
    "    # Test 1: Verify we can list objects in our S3 bucket\n",
    "    print(\"\\n1Ô∏è‚É£ Testing S3 bucket access...\")\n",
    "    bucket_objects = make_api_request(\"GET\", f\"/s3/objects/{workflow_bucket}\")\n",
    "    \n",
    "    if \"error\" not in bucket_objects and \"objects\" in bucket_objects:\n",
    "        objects = bucket_objects[\"objects\"]\n",
    "        print(f\"‚úÖ S3 bucket contains {len(objects)} objects\")\n",
    "        for obj in objects:\n",
    "            print(f\"   üìÑ {obj['key']} ({obj['size']} bytes)\")\n",
    "    else:\n",
    "        print(\"‚ùå Could not access S3 bucket objects\")\n",
    "    \n",
    "    # Test 2: Test one of the presigned URLs\n",
    "    print(\"\\n2Ô∏è‚É£ Testing presigned URL access...\")\n",
    "    if files_with_urls:\n",
    "        test_file = files_with_urls[0]  # Test first file\n",
    "        test_url = test_file[\"presigned_url\"]\n",
    "        \n",
    "        print(f\"üîó Testing access to: {test_file['filename']}\")\n",
    "        \n",
    "        try:\n",
    "            # Test the presigned URL directly (without authentication)\n",
    "            response = requests.get(test_url)\n",
    "            if response.status_code == 200:\n",
    "                print(f\"‚úÖ Presigned URL works! Downloaded {len(response.content)} bytes\")\n",
    "                \n",
    "                # Show preview of content if it's text\n",
    "                if test_file[\"format\"] in [\"CSV\", \"Markdown\", \"JSON\"]:\n",
    "                    content_preview = response.text[:200]\n",
    "                    print(f\"üìÑ Content preview:\\n{content_preview}...\")\n",
    "            else:\n",
    "                print(f\"‚ùå Presigned URL failed: {response.status_code}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error testing presigned URL: {e}\")\n",
    "    \n",
    "    # Test 3: Verify dataset metadata\n",
    "    print(\"\\n3Ô∏è‚É£ Workflow verification summary...\")\n",
    "    print(f\"‚úÖ Created S3 bucket: {workflow_bucket}\")\n",
    "    print(f\"‚úÖ Uploaded {len(uploaded_files)} research files\")\n",
    "    print(f\"‚úÖ Generated {len(files_with_urls)} presigned URLs\")\n",
    "    print(f\"‚úÖ Registered dataset: {workflow_dataset_id}\")\n",
    "    print(f\"‚úÖ Dataset includes {len(dataset_resources)} S3 resources\")\n",
    "    \n",
    "    total_size_mb = sum(f.get('size', 0) for f in uploaded_files) / (1024*1024)\n",
    "    print(f\"üìä Total data stored: {total_size_mb:.2f} MB\")\n",
    "    print(f\"üîó All files accessible via presigned URLs for {presigned_request['expires_in']//3600} hours\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Workflow incomplete - cannot verify\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Alternative: Create Direct S3 URLs (Optional)\n",
    "\n",
    "For reference, here's how to create \"permanent\" URLs using direct S3 access (requires bucket to be public)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if workflow_bucket and uploaded_files:\n",
    "    print(\"üìö Alternative approach: Direct S3 URLs\")\n",
    "    print(\"(Note: This requires the bucket to be configured as public)\")\n",
    "    \n",
    "    direct_urls = []\n",
    "    for file_info in uploaded_files:\n",
    "        # Direct S3 URL format (works if bucket is public)\n",
    "        direct_url = f\"http://localhost:9000/{file_info['bucket']}/{file_info['key']}\"\n",
    "        direct_urls.append({\n",
    "            \"filename\": file_info[\"filename\"],\n",
    "            \"direct_url\": direct_url,\n",
    "            \"description\": file_info[\"description\"]\n",
    "        })\n",
    "        \n",
    "        print(f\"üîó {file_info['filename']}: {direct_url}\")\n",
    "    \n",
    "    print(\"\\nüí° Direct URLs vs Presigned URLs:\")\n",
    "    print(\"   ‚Ä¢ Direct URLs: Permanent but require public bucket\")\n",
    "    print(\"   ‚Ä¢ Presigned URLs: Temporary but work with private buckets\")\n",
    "    print(\"   ‚Ä¢ For research data, presigned URLs are usually preferred for security\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No files available for direct URL demonstration\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
